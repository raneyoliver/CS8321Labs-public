{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "Group Members:\n",
    "\n",
    "- Parker\n",
    "\n",
    "- Suma \n",
    "\n",
    "- Chris\n",
    "\n",
    "- Oliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Why is it important to find this kind of bias in machine learning models?**\n",
    "\n",
    "TODO: Answer this\n",
    "\n",
    "**Why will the type of investigation I am performing be relevant to other researchers or practitioners?**\n",
    "\n",
    "TODO: Answer this\n",
    "\n",
    "For example, we are trying to answer if a romantic comedy is ranked as more positive or if a horror movie is ranked as more negative. \n",
    "\n",
    "To do this, we utilize the dataset of IMDB reviews that are pre-labeled as positive or negative and contain movie genres.\n",
    "We will analyze the bias using the word embeddings of GloVe & ConceptNet. Each of these embeddings will be applied to the movie reviews to determine the overall sentiment of the review. \n",
    "\n",
    "We believe that a \"lesser\" embedding will perform more poorly in the face of a conflicting sentiment lexicon. By this, we expect that \"horror\" movies may have more \"negative\" ratings because of the \"negative\" words used in the reviews to describe the content of the movie, versus the overall context of the review as positive or negative. If the embedding is more narrow-focused, like Glove, it may produce results that bias towards romantic comedies as more positive, since the overall content of the review should have more \"positive\" words based on the content of the movie. However, we expect an embedding that has a wider knowledge graph focus, like ConceptNet, to remove this bias and focus solely on the review content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embeddings and Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is taken from LectureNotesMaster/01 ConceptNet.ipynb notebook shared by Prof E Larson/Robyn Speer\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('data/glove.840B.300d.txt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the lexicon\n",
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('data/positive-words.txt')\n",
    "neg_words = load_lexicon('data/negative-words.txt')\n",
    "\n",
    "print(len(pos_words), len(neg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "pos_words_common = list(set(pos_words) & set(embeddings.index)) \n",
    "neg_words_common = list(set(neg_words) & set(embeddings.index)) \n",
    "\n",
    "pos_vectors = embeddings.loc[pos_words_common]\n",
    "neg_vectors = embeddings.loc[neg_words_common]\n",
    "print(pos_vectors.shape,neg_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the inputs and outputs\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for train test split\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear classifier \n",
    "model = SGDClassifier(loss='log_loss', random_state=0, max_iter=100)\n",
    "model.fit(train_vectors, train_targets)\n",
    "accuracy_score(model.predict(test_vectors), test_targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    # this is a logarithm of the max margin for the classifier, \n",
    "    # similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize using regular expressions\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    # tokenize the input phrase\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    # send each token separately into the embedding, then the classifier\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean() # return the mean for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the movie list for Comedy\n",
    "df_cat1_movies = pd.read_csv('data/movie_dataset/1_movies_per_genre/Comedy.csv', sep = ',')\n",
    "df_cat1_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2_movies = pd.read_csv('data/movie_dataset/1_movies_per_genre/Horror.csv', sep = ',')\n",
    "df_cat2_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I feel name, year and genres cols in the csv files are sufficient to retain, drop the rest, add cols back in as needed\n",
    "df_cat1_movies_lim=df_cat1_movies[['name', 'year',  'genres']]\n",
    "df_cat2_movies_lim=df_cat2_movies[['name', 'year', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting  problem - there is movie titled Evil Dead thats a Comedy;Horror mix. May be we need load Cat1 to some other genre?\n",
    "\n",
    "For the movies in the dataframes we have loaded, bring in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_movies = pd.concat([df_cat1_movies_lim, df_cat2_movies_lim])\n",
    "df_concat_movies['file_name'] = df_concat_movies['name'] + ' ' + df_concat_movies['year'].astype(str) + '.csv'\n",
    "\n",
    "df_concat_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/how-to-iterate-over-files-in-directory-using-python/\n",
    "# and https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "import os\n",
    "directory = 'data/movie_dataset/2_reviews_per_movie_raw'\n",
    "\n",
    "dfs = list()\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(directory):\t\n",
    "\tprint('looking for :', filename) \n",
    "\tif df_concat_movies['file_name'].eq(filename).any():\n",
    "\t\tprint(\"Found:\", df_concat_movies['file_name'])\n",
    "\t\tdata = pd.read_csv(os.path.join(directory, filename), header=None)\n",
    "\t\tdata['file_name'] = filename\n",
    "#\t\tdata['genre'] = df_concat_movies['genres']        \n",
    "\t\tdfs.append(data)\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df = df.rename(columns={\"filename\": \"file_name\"})\n",
    "df_all = df_concat_movies.merge(df)\n",
    "df_all.columns = df_all.iloc[0]\n",
    "df_all = df_all[1:]\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "# Fixing a weird result from the merge - this seems to work ok\n",
    "df_all = df_all.rename(columns={\"Guardians of the Galaxy\": \"film name\", 2014: \"year\", 'Comedy; ': \"genre\", \"Guardians of the Galaxy 2014.csv\": \"file_name\"})\n",
    "df_all = df_all.drop(columns=['file_name','username', 'helpful', 'total', 'date','title'])\n",
    "df_all['genre'] = df_all['genre'].str.replace(';','',regex=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('df_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'Comedy' genre only (excluding rows that also contain 'Horror')\n",
    "df_romcom = df_all[df_all['genre'].str.contains('Comedy') & ~df_all['genre'].str.contains('Horror')]\n",
    "\n",
    "# Filter for 'Horror' genre only (excluding rows that also contain 'Comedy')\n",
    "df_horror = df_all[df_all['genre'].str.contains('Horror') & ~df_all['genre'].str.contains('Comedy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_romcom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horror.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 174941 rows of movie reviews, with 5 columns of information: \n",
    "- Film Name\n",
    "- Year\n",
    "- Genre\n",
    "- Rating\n",
    "- Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: We need to add the header in to the appended reviews file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of your assignment, you will choose a methodology that involves comparing two (or more) techniques to one another. Discuss how you will measure a difference between the two techniques. That is, if you are measuring the difference statistically, what test will you use and why is it appropriate? Are there any limitations to performing this test that you should be aware of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has 1 million reviews from 1150 movies spread across 17 genres; there's also other meta data such as the IMDb rating and movie rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Paper: https://ieeexplore-ieee-org.proxy.libraries.smu.edu/document/9276893 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric\n",
    "\n",
    "F-score\n",
    "\n",
    "TODO: Explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "TODO: Write hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carryout your analysis and model training. Explain your steps in as much detail so that the instructor can understand your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present results from your analysis and provide evidence from the results that support or refute your hypothesis. Write a conclusion based upon the various analyses you performed. Be sure to reference your research questions systematically in your conclusion. With your analysis complete, are there any additional research questions or limitations to your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning/preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TODO: Add conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify two conferences or journals that would be interested in the results of your analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
